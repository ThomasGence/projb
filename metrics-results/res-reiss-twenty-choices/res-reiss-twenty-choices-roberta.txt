---
language:
- en
license: mit
tags:
- generated_from_trainer
datasets:
- swag
metrics:
- accuracy
model-index:
- name: reiss_twenty_choices_roberta
  results:
  - task:
      name: Multiple Choice
      type: multiple-choice
    dataset:
      name: SWAG
      type: swag
      args: regular
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.2485920322025986
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# reiss_twenty_choices_roberta

This model is a fine-tuned version of [roberta-base](https://huggingface.co/roberta-base) on the "wanagenst/reiss-twenty-choices" dataset

It achieves the following results on the evaluation set:
- Loss: 1.1565
- Accuracy: 0.2486
- Report:               precision    recall  f1-score   support

    	   ending0       0.77        0.86      0.81      3602
   	   ending1       0.05        0.01      0.02        99
   	   ending2       0.00        0.00      0.00         5
   	   ending3       0.41        0.43      0.42       139
   	   ending4       0.46        0.29      0.36       184
   	   ending5       0.32        0.26      0.29       134
   	   ending6       0.38        0.45      0.41       173
     	   ending7       0.45        0.61      0.52       140
     	   ending8       0.38        0.34      0.36        97
     	   ending9       0.00        0.00      0.00        27
    	   ending10      0.00        0.00      0.00        26
    	   ending11      0.24        0.05      0.09        76
    	   ending12      0.24        0.11      0.15       137
    	   ending13      0.12        0.04      0.06        51
    	   ending14      0.24        0.21      0.22        34
    	   ending15      0.44        0.55      0.49        82
    	   ending16      0.44        0.35      0.39       134
    	   ending17      0.33        0.03      0.06        30
    	   ending18      0.21        0.08      0.12       120
    	   ending19      0.30        0.29      0.29       128

    	   accuracy                            0.67      5418
	   macro avg     0.29        0.25      0.25      5418
	   weighted avg  0.62        0.67      0.64      5418


## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 12
- eval_batch_size: 32
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3.0

### Training results



### Framework versions

- Transformers 4.16.0.dev0
- Pytorch 1.10.1+cu102
- Datasets 1.17.0
- Tokenizers 0.10.3
